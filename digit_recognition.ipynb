{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit-recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanDas3/reconhecimento-digitos/blob/master/digit_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQLTRyjBBgQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding: utf-8\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "import keras #Rede Neural\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from getpass import getpass\n",
        "from os import listdir\n",
        "from os.path import isfile, join"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBIe9lDCCN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"reconhecimento-digitos\"\n",
        "data_root = \"data\"\n",
        "out_path_root = \"output\"\n",
        "digits_trains = \"cvl-single-digits-train-validation/train\"\n",
        "digits_eval = \"cvl-single-digits-train-validation/valid\"\n",
        "digits_valid = \"cvl-single-digits-completeDatabase/cvl-single-digits/valid/\"\n",
        "string_trains = \"cvl-strings-train/train\"\n",
        "string_eval = \"cvl-strings-eval/cvl-strings-eval\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOkJZGaiCJc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "32878a26-e541-4ea9-ba18-0492ba7ccef9"
      },
      "source": [
        "if os.path.exists(root) == False:\n",
        "  !git clone https://github.com/DanDas3/reconhecimento-digitos.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'reconhecimento-digitos'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/49)\u001b[K\rremote: Counting objects:   4% (2/49)\u001b[K\rremote: Counting objects:   6% (3/49)\u001b[K\rremote: Counting objects:   8% (4/49)\u001b[K\rremote: Counting objects:  10% (5/49)\u001b[K\rremote: Counting objects:  12% (6/49)\u001b[K\rremote: Counting objects:  14% (7/49)\u001b[K\rremote: Counting objects:  16% (8/49)\u001b[K\rremote: Counting objects:  18% (9/49)\u001b[K\rremote: Counting objects:  20% (10/49)\u001b[K\rremote: Counting objects:  22% (11/49)\u001b[K\rremote: Counting objects:  24% (12/49)\u001b[K\rremote: Counting objects:  26% (13/49)\u001b[K\rremote: Counting objects:  28% (14/49)\u001b[K\rremote: Counting objects:  30% (15/49)\u001b[K\rremote: Counting objects:  32% (16/49)\u001b[K\rremote: Counting objects:  34% (17/49)\u001b[K\rremote: Counting objects:  36% (18/49)\u001b[K\rremote: Counting objects:  38% (19/49)\u001b[K\rremote: Counting objects:  40% (20/49)\u001b[K\rremote: Counting objects:  42% (21/49)\u001b[K\rremote: Counting objects:  44% (22/49)\u001b[K\rremote: Counting objects:  46% (23/49)\u001b[K\rremote: Counting objects:  48% (24/49)\u001b[K\rremote: Counting objects:  51% (25/49)\u001b[K\rremote: Counting objects:  53% (26/49)\u001b[K\rremote: Counting objects:  55% (27/49)\u001b[K\rremote: Counting objects:  57% (28/49)\u001b[K\rremote: Counting objects:  59% (29/49)\u001b[K\rremote: Counting objects:  61% (30/49)\u001b[K\rremote: Counting objects:  63% (31/49)\u001b[K\rremote: Counting objects:  65% (32/49)\u001b[K\rremote: Counting objects:  67% (33/49)\u001b[K\rremote: Counting objects:  69% (34/49)\u001b[K\rremote: Counting objects:  71% (35/49)\u001b[K\rremote: Counting objects:  73% (36/49)\u001b[K\rremote: Counting objects:  75% (37/49)\u001b[K\rremote: Counting objects:  77% (38/49)\u001b[K\rremote: Counting objects:  79% (39/49)\u001b[K\rremote: Counting objects:  81% (40/49)\u001b[K\rremote: Counting objects:  83% (41/49)\u001b[K\rremote: Counting objects:  85% (42/49)\u001b[K\rremote: Counting objects:  87% (43/49)\u001b[K\rremote: Counting objects:  89% (44/49)\u001b[K\rremote: Counting objects:  91% (45/49)\u001b[K\rremote: Counting objects:  93% (46/49)\u001b[K\rremote: Counting objects:  95% (47/49)\u001b[K\rremote: Counting objects:  97% (48/49)\u001b[K\rremote: Counting objects: 100% (49/49)\u001b[K\rremote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 79494 (delta 17), reused 1 (delta 0), pack-reused 79445\u001b[K\n",
            "Receiving objects: 100% (79494/79494), 187.79 MiB | 38.73 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "Checking out files: 100% (93528/93528), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epvhmoDUCVyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funções\n",
        "\n",
        "def convertBin(img):\n",
        "  max_value = 255\n",
        "  neighborhood_size=99\n",
        "  subtract_from_mean = 10\n",
        "  return cv.adaptiveThreshold(img, max_value, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, neighborhood_size, subtract_from_mean)\n",
        "  #return cv.threshold(img, 200, 255, cv.THRESH_BINARY)\n",
        "\n",
        "def segmenta(img):\n",
        "  linha = img.shape[0]\n",
        "  coluna = img.shape[1]\n",
        "  \n",
        "  # armazena uma tupla com o inicio e fim do dígito\n",
        "  posicoes = []\n",
        "\n",
        "  inicioBool = False\n",
        "  #fimBool = False\n",
        "  xInicial = 0\n",
        "  xFinal = 0\n",
        "  countPreto = 0\n",
        "\n",
        "  for x in range(coluna):\n",
        "    for y in range(linha):\n",
        "      if(pb[y][x] == 0):\n",
        "        if (inicioBool == False):\n",
        "          inicioBool = True\n",
        "          xInicial = x;\n",
        "\n",
        "        countPreto += 1\n",
        "    if(countPreto == 0 and inicioBool == True):\n",
        "        xFinal = x\n",
        "        inicioBool = False\n",
        "        posicoes.append(((xInicial,0+5), (xFinal, pb.shape[0]-5)))  \n",
        "    countPreto = 0\n",
        "  return posicoes\n",
        "\n",
        "def marcarSegmentos(img, positions):\n",
        "  for position in positions:\n",
        "    cv.rectangle(img, position[0],position[1],(0,0,0),thickness=2)\n",
        "  return img\n",
        "\n",
        "def carregaDigitTrain():\n",
        "  path = root + \"/\" + data_root + \"/\" + digits_trains + \"/\"\n",
        "  images = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "  data = np.ndarray((len(images), 32, 32, 3))\n",
        "  tags = np.ndarray((len(images), 1))\n",
        "  for i in range(len(images)):\n",
        "    img = cv.imread(path + images[i])\n",
        "    h_medio += img.shape[0]\n",
        "    w_medio += img.shape[1]\n",
        "    img = cv.resize(img,(32,32))\n",
        "    tags[i] = int(images[i][:1])\n",
        "    data[i] = img\n",
        "  return data, tags\n",
        "\n",
        "def carregaDigitEval():\n",
        "  path = root + \"/\" + data_root + \"/\" + digits_eval + \"/\"\n",
        "  images = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "  data = np.ndarray((len(images), 32, 32, 3))\n",
        "  tags = np.ndarray((len(images), 1))\n",
        "  for i in range(len(images)):\n",
        "    img = cv.imread(path + images[i])\n",
        "    img = cv.resize(img,(32,32))\n",
        "    tags[i] = int(images[i][:1])\n",
        "    data[i] = img\n",
        "  \n",
        "  return data, tags\n",
        "\n",
        "def carregaDigitValid():\n",
        "  path = root + \"/\" + data_root + \"/\" + digits_valid + \"/\"\n",
        "  images = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "  data = np.ndarray((len(images), 32, 32, 3))\n",
        "  tags = np.ndarray((len(images), 1))\n",
        "  for i in range(len(images)):\n",
        "    img = cv.imread(path + images[i])\n",
        "    img = cv.resize(img,(32,32))\n",
        "    tags[i] = int(images[i][:1])\n",
        "    data[i] = img\n",
        "  return data, tags\n",
        "\n",
        "def carregaStringsValid():\n",
        "  path = root + \"/\" + data_root + \"/\" + string_eval + \"/\"\n",
        "  images = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "  data = np.ndarray((len(images), 32, 32, 3))\n",
        "  tags = np.ndarray((len(images), 1))\n",
        "  for i in range(len(images)):\n",
        "    img = cv.imread(path + images[i])\n",
        "    img = cv.resize(img,(365,89))    \n",
        "    tag_num = images[i].split(\"-\",1)[0]\n",
        "    tags[i] = int(tag_num)\n",
        "    data[i] = img\n",
        "  \n",
        "  return data, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuhodTnHCqib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rede Neural\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 1000\n",
        "\n",
        "# Load data and target vector from movie review data\n",
        "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
        "num_words=number_of_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP_o-HRMHBy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Divide em treinamento, teste e validação\n",
        "(data_train, data_valid) = data_train[number_of_features//2:], data_train[:number_of_features//2]\n",
        "(target_train, target_valid) = target_train[number_of_features//2:], target_train[:number_of_features//2]\n",
        "# Convert movie review data to one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features//2)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_valid = tokenizer.sequences_to_matrix(data_valid, mode=\"binary\")\n",
        "\n",
        "data_test = data_test[:500]\n",
        "target_test = target_test[:500]\n",
        "tokenizer = Tokenizer(num_words=number_of_features//2)\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5xT1tFdHJFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features//2,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation=\"sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwHBb9n3HR91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile neural network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
        "optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "metrics=[\"accuracy\"]) # Accuracy performance metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2JY5blnHYpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9915f266-1406-42db-97e1-2e7b20602142"
      },
      "source": [
        "# Train neural network\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint   \n",
        "\n",
        "# Salva o melhor resultado\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "history = network.fit(features_train, # Features\n",
        "target_train, # Target vector\n",
        "epochs=20, # Number of epochs\n",
        "verbose=2, # Print description after each epoch\n",
        "batch_size=10, # Number of observations per batch\n",
        "validation_data=(features_valid, target_valid), # Validation data\n",
        "callbacks=[checkpointer],\n",
        "shuffle=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 24500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            " - 7s - loss: 0.3699 - acc: 0.8433 - val_loss: 0.3579 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.35786, saving model to model.weights.best.hdf5\n",
            "Epoch 2/20\n",
            " - 7s - loss: 0.3627 - acc: 0.8460 - val_loss: 0.3538 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.35786 to 0.35381, saving model to model.weights.best.hdf5\n",
            "Epoch 3/20\n",
            " - 8s - loss: 0.3547 - acc: 0.8524 - val_loss: 0.3785 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.35381\n",
            "Epoch 4/20\n",
            " - 8s - loss: 0.3470 - acc: 0.8577 - val_loss: 0.3731 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.35381\n",
            "Epoch 5/20\n",
            " - 8s - loss: 0.3397 - acc: 0.8637 - val_loss: 0.4094 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.35381\n",
            "Epoch 6/20\n",
            " - 8s - loss: 0.3330 - acc: 0.8682 - val_loss: 0.3988 - val_acc: 0.8240\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.35381\n",
            "Epoch 7/20\n",
            " - 7s - loss: 0.3242 - acc: 0.8719 - val_loss: 0.4243 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.35381\n",
            "Epoch 8/20\n",
            " - 7s - loss: 0.3173 - acc: 0.8771 - val_loss: 0.4455 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.35381\n",
            "Epoch 9/20\n",
            " - 7s - loss: 0.3115 - acc: 0.8808 - val_loss: 0.4528 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.35381\n",
            "Epoch 10/20\n",
            " - 8s - loss: 0.3059 - acc: 0.8850 - val_loss: 0.4449 - val_acc: 0.8300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.35381\n",
            "Epoch 11/20\n",
            " - 7s - loss: 0.3032 - acc: 0.8856 - val_loss: 0.4327 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.35381\n",
            "Epoch 12/20\n",
            " - 8s - loss: 0.2967 - acc: 0.8880 - val_loss: 0.4369 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.35381\n",
            "Epoch 13/20\n",
            " - 8s - loss: 0.2943 - acc: 0.8911 - val_loss: 0.4213 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.35381\n",
            "Epoch 14/20\n",
            " - 7s - loss: 0.2908 - acc: 0.8942 - val_loss: 0.4378 - val_acc: 0.8240\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.35381\n",
            "Epoch 15/20\n",
            " - 7s - loss: 0.2862 - acc: 0.8945 - val_loss: 0.4419 - val_acc: 0.8280\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.35381\n",
            "Epoch 16/20\n",
            " - 7s - loss: 0.2822 - acc: 0.8966 - val_loss: 0.4872 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.35381\n",
            "Epoch 17/20\n",
            " - 8s - loss: 0.2766 - acc: 0.8995 - val_loss: 0.4703 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.35381\n",
            "Epoch 18/20\n",
            " - 7s - loss: 0.2749 - acc: 0.9011 - val_loss: 0.4719 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.35381\n",
            "Epoch 19/20\n",
            " - 8s - loss: 0.2686 - acc: 0.9040 - val_loss: 0.5700 - val_acc: 0.8280\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.35381\n",
            "Epoch 20/20\n",
            " - 8s - loss: 0.2658 - acc: 0.9050 - val_loss: 0.5549 - val_acc: 0.8260\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.35381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBZYLctNMKxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carrega os pesos com melho precisão na validação\n",
        "network.load_weights('model.weights.best.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuOrb4bpOb2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5b237881-5980-4e60-a1a8-2dbe96addeb1"
      },
      "source": [
        "print(features_test.shape)\n",
        "print(features_test[0].shape)\n",
        "print(features_test[0:1].shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 500)\n",
            "(500,)\n",
            "(1, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3y9hqnYM4OD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ca0e19d6-074a-470d-c4e9-a1ab27e3e137"
      },
      "source": [
        "# Avaliaçãão do treinamento\n",
        "score = network.evaluate(features_test, target_test, verbose=0)\n",
        "print(\"\\n\",\"Resultado:\", score[1])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Resultado: 0.8400000009536743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxClsW57S_Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = network.predict(features_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP-5a5EaTLIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "27fcf2bf-25a8-4052-a9c6-ce2e83012118"
      },
      "source": [
        "print(type(data_test[0]))\n",
        "print(type(target_test[0]))\n",
        "print(target_test[2])\n",
        "for i, idx in enumerate(np.random.choice(features_test.shape[0], size=32, replace=False)):\n",
        "  pred_idx = np.argmax(y_hat[idx])\n",
        "  true_idx = np.argmax(data_test[idx])\n",
        "\n",
        "  print(data_test[true_idx] == data_test[pred_idx] )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'numpy.int64'>\n",
            "1\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}